{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Shuffle and Split bins</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data provided by UOC includes data from Jul 2019 until Dec 2019.  \n",
    "  \n",
    "The purpose of this notebook is:  \n",
    "- Generate data from Jan 2019 until Jun 2019  \n",
    "- Split original data (months) in smaller datasets   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, glob, json, os\n",
    "import pandas as pd\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducible results\n",
    "random_state = 33\n",
    "# if debug is True, messages during the process are generated\n",
    "debug = False\n",
    "# if the test is True, the process runs on a smaller subset of raw data (json files)\n",
    "test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the doc & meta csv files are stored\n",
    "if test:\n",
    "    path = r'D:\\master\\data science\\semestre 4\\M2.979 - tfm\\data\\bins_test'\n",
    "else:\n",
    "    path = r'D:\\master\\data science\\semestre 4\\M2.979 - tfm\\data\\bins'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to shuffle and split months/bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:  \n",
    "- From the input dataset \\data\\bins\\doc\\2019_Dec (original number of rows)  \n",
    "- Generate the output datasets \\data\\bins\\doc\\2019_Dec (original number of rows / 2)  \n",
    "- Generate the output datasets \\data\\bins\\doc\\2019_Jun (original number of rows / 2)  \n",
    "  \n",
    "  \n",
    "- From the input dataset \\data\\bins\\meta\\2019_Dec (original number of rows)  \n",
    "- Generate the output datasets \\data\\bins\\meta\\2019_Dec (original number of rows / 2)  \n",
    "- Generate the output datasets \\data\\bins\\meta\\2019_Jun (original number of rows / 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2):\n",
    "\n",
    "    # path + name of the file that contains the documents\n",
    "    path_1 = os.path.join(path, 'doc', yyyy_MMM_1)\n",
    "    file_1 = os.path.join(path, 'doc', yyyy_MMM_1, yyyy_MMM_1)\n",
    "\n",
    "    # path + name of the file that contains the documents\n",
    "    path_2 = os.path.join(path, 'doc', yyyy_MMM_2)\n",
    "    file_2 = os.path.join(path, 'doc', yyyy_MMM_2, yyyy_MMM_2)\n",
    "    \n",
    "    # load documents\n",
    "    df_doc = pd.read_csv(file_1 + '_original.csv', names=['id_doc', 'content'])\n",
    "    # shuffle dataset\n",
    "    df_doc = df_doc.sample(frac=1, random_state=random_state)\n",
    "    # number of rows in dataset\n",
    "    num_rows = len(df_doc)\n",
    "\n",
    "    # split doc dataset (first half)\n",
    "    df_doc_head = df_doc.head(int(num_rows/2))\n",
    "    df_doc_head.to_csv(file_1 + '.csv', index=False, header=False)\n",
    "    \n",
    "    # split doc dataset (second half)\n",
    "    df_doc_tail = df_doc.tail(int(num_rows/2))\n",
    "    # check if document subfolder doc\\yyyy_mmm exists, if not create it\n",
    "    if not os.path.exists(os.path.join(path_2)):\n",
    "        os.makedirs(os.path.join(path_2))\n",
    "    df_doc_tail.to_csv(file_2 + '.csv', index=False, header=False)\n",
    "    \n",
    "    # path + name of the file that contains the metadata\n",
    "    path_1 = os.path.join(path, 'meta', yyyy_MMM_1)\n",
    "    file_1 = os.path.join(path, 'meta', yyyy_MMM_1, yyyy_MMM_1)\n",
    "\n",
    "    # path + name of the file that contains the metadata\n",
    "    path_2 = os.path.join(path, 'meta', yyyy_MMM_2)\n",
    "    file_2 = os.path.join(path, 'meta', yyyy_MMM_2, yyyy_MMM_2)\n",
    "    \n",
    "    # split meta dataset (first half)\n",
    "    df_meta = pd.read_csv(file_1 + '_original.csv', names=['id_meta', 'file', 'author_followers', 'author_full_name', 'author_id', 'author_image', 'author_name', 'author_url', 'date', 'date_from_provider', 'id', 'id_from_provider', 'image_url', 'link', 'location_latitude', 'location_longitude', 'place_country_code', 'place_name', 'place_street_address', 'provider', 'social_likes', 'social_replies'])\n",
    "    df_meta_head = pd.merge(left=df_doc_head, right=df_meta, left_on='id_doc', right_on='id_meta')\n",
    "    del df_meta_head['id_doc']\n",
    "    del df_meta_head['content']\n",
    "    df_meta_head.to_csv(file_1 + '.csv', index=False, header=False)\n",
    "    \n",
    "    # split meta dataset (second half)\n",
    "    df_meta_tail = pd.merge(left=df_doc_tail, right=df_meta, left_on='id_doc', right_on='id_meta')\n",
    "    del df_meta_tail['id_doc']\n",
    "    del df_meta_tail['content']\n",
    "    # check if document subfolder meta\\yyyy_mmm exists, if not create it\n",
    "    if not os.path.exists(os.path.join(path_2)):\n",
    "        os.makedirs(os.path.join(path_2))\n",
    "    df_meta_tail.to_csv(file_2 + '.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split months/bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **2019_Jul_original** generate **2019_Jul** and **2019_Jan**  \n",
    "From **2019_Aug_original** generate **2019_Aug** and **2019_Feb**  \n",
    "From **2019_Sep_original** generate **2019_Sep** and **2019_Mar**  \n",
    "From **2019_Oct_original** generate **2019_Oct** and **2019_Apr**  \n",
    "From **2019_Nov_original** generate **2019_Nov** and **2019_May**  \n",
    "From **2019_Dec_original** generate **2019_Dec** and **2019_Jun**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyyy_MMM_1 = '2019_Jul'\n",
    "yyyy_MMM_2 = '2019_Jan'\n",
    "shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2)\n",
    "\n",
    "yyyy_MMM_1 = '2019_Aug'\n",
    "yyyy_MMM_2 = '2019_Feb'\n",
    "shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2)\n",
    "\n",
    "yyyy_MMM_1 = '2019_Sep'\n",
    "yyyy_MMM_2 = '2019_Mar'\n",
    "shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2)\n",
    "\n",
    "yyyy_MMM_1 = '2019_Oct'\n",
    "yyyy_MMM_2 = '2019_Apr'\n",
    "shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2)\n",
    "\n",
    "yyyy_MMM_1 = '2019_Nov'\n",
    "yyyy_MMM_2 = '2019_May'\n",
    "shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2)\n",
    "\n",
    "yyyy_MMM_1 = '2019_Dec'\n",
    "yyyy_MMM_2 = '2019_Jun'\n",
    "shuffle_split(path, yyyy_MMM_1, yyyy_MMM_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
